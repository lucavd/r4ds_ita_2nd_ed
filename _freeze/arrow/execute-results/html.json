{
  "hash": "b8efa91c90012e43cfa1d68484ce6b5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfreeze: true\n---\n\n\n\n\n\n# Arrow {#sec-arrow}\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Introduzione\n\nI file CSV sono progettati per essere facilmente letti dagli esseri umani.\nSono un buon formato di interscambio perché sono molto semplici e possono essere letti da qualsiasi strumento esistente.\nMa i file CSV non sono molto efficienti: devi fare parecchio lavoro per leggere i dati in R.\nIn questo capitolo, imparerai un'alternativa potente: il [formato parquet](https://parquet.apache.org/), un formato basato su standard aperti ampiamente utilizzato dai sistemi di big data.\n\nAbbineremo i file parquet con [Apache Arrow](https://arrow.apache.org), una toolbox multi-linguaggio progettata per l'analisi efficiente e il trasporto di grandi dataset.\nUseremo Apache Arrow tramite il [pacchetto arrow](https://arrow.apache.org/docs/r/), che fornisce un backend dplyr permettendoti di analizzare dataset più grandi della memoria usando la sintassi familiare di dplyr.\nCome beneficio aggiuntivo, arrow è estremamente veloce: vedrai alcuni esempi più avanti nel capitolo.\n\nSia arrow che dbplyr forniscono backend dplyr, quindi potresti chiederti quando usare ciascuno.\nIn molti casi, la scelta è fatta per te, poiché i dati sono già in un database o in file parquet, e vorrai lavorarci così come sono.\nMa se stai iniziando con i tuoi dati (magari file CSV), puoi caricarli in un database o convertirli in parquet.\nIn generale, è difficile sapere cosa funzionerà meglio, quindi nelle fasi iniziali della tua analisi ti incoraggiamo a provare entrambi e scegliere quello che funziona meglio per te.\n\n(Un grande ringraziamento a Danielle Navarro che ha contribuito alla versione iniziale di questo capitolo.)\n\n### Prerequisiti\n\nIn questo capitolo, continueremo a usare il tidyverse, in particolare dplyr, ma lo abbineremo al pacchetto arrow che è progettato specificamente per lavorare con grandi dati.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\n```\n:::\n\n\n\n\n\nPiù avanti nel capitolo, vedremo anche alcune connessioni tra arrow e duckdb, quindi avremo bisogno anche di dbplyr e duckdb.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n#> Loading required package: DBI\n```\n:::\n\n\n\n\n\n## Ottenere i dati\n\nIniziamo ottenendo un dataset degno di questi strumenti: un dataset di prestiti di oggetti dalle biblioteche pubbliche di Seattle, disponibile online su [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nQuesto dataset contiene 41.389.465 righe che ti dicono quante volte ogni libro è stato preso in prestito ogni mese da aprile 2005 a ottobre 2022.\n\nIl seguente codice ti darà una copia cached dei dati.\nI dati sono un file CSV di 9GB, quindi ci vorrà del tempo per scaricarlo.\nRaccomando vivamente di usare `curl::multi_download()` per ottenere file molto grandi poiché è costruito esattamente per questo scopo: ti dà una barra di progresso e può riprendere il download se viene interrotto.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n#> # A tibble: 1 × 10\n#>   success status_code resumefrom url                    destfile        error\n#>   <lgl>         <dbl>      <dbl> <chr>                  <chr>           <chr>\n#> 1 TRUE            200          0 https://r4ds.s3.us-we… /Users/utente/… <NA> \n#> # ℹ 4 more variables: type <chr>, modified <dttm>, time <dbl>,\n#> #   headers <list>\n```\n:::\n\n\n\n\n\n## Aprire un dataset\n\nIniziamo dando un'occhiata ai dati.\nA 9 GB, questo file è abbastanza grande che probabilmente non vogliamo caricare tutto in memoria.\nUna buona regola empirica è che di solito vuoi almeno il doppio della memoria rispetto alla dimensione dei dati, e molti laptop arrivano al massimo a 16 GB.\nQuesto significa che vogliamo evitare `read_csv()` e invece usare `arrow::open_dataset()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n```\n:::\n\n\n\n\n\nCosa succede quando questo codice viene eseguito?\n`open_dataset()` scannerà alcune migliaia di righe per capire la struttura del dataset.\nLa colonna `ISBN` contiene valori vuoti per le prime 80.000 righe, quindi dobbiamo specificare il tipo di colonna per aiutare arrow a capire la struttura dei dati.\nUna volta che i dati sono stati scansionati da `open_dataset()`, registra quello che ha trovato e si ferma; leggerà solo ulteriori righe quando le richiedi specificamente.\nQuesti metadati sono quello che vediamo se stampiamo `seattle_csv`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv\n#> FileSystemDataset with 1 csv file\n#> 12 columns\n#> UsageClass: string\n#> CheckoutType: string\n#> MaterialType: string\n#> CheckoutYear: int64\n#> CheckoutMonth: int64\n#> Checkouts: int64\n#> Title: string\n#> ISBN: string\n#> Creator: string\n#> Subjects: string\n#> Publisher: string\n#> PublicationYear: string\n```\n:::\n\n\n\n\n\nLa prima riga nell'output ti dice che `seattle_csv` è memorizzato localmente su disco come un singolo file CSV; sarà caricato in memoria solo quando necessario.\nIl resto dell'output ti dice il tipo di colonna che arrow ha imputato per ogni colonna.\n\nPossiamo vedere cosa c'è effettivamente dentro con `glimpse()`.\nQuesto rivela che ci sono ~41 milioni di righe e 12 colonne, e ci mostra alcuni valori.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#> $ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#> $ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#> $ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#> $ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#> $ Title           <string> \"Super rich : a guide to having it all / Russell S…\n#> $ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#> $ Subjects        <string> \"Self realization, Conduct of life, Attitude Psych…\n#> $ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#> $ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n```\n:::\n\n\n\n\n\nPossiamo iniziare a usare questo dataset con i verbi dplyr, usando `collect()` per forzare arrow a eseguire il calcolo e restituire alcuni dati.\nPer esempio, questo codice ci dice il numero totale di prestiti per anno:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  group_by(CheckoutYear) |> \n  summarise(Checkouts = sum(Checkouts)) |> \n  arrange(CheckoutYear) |> \n  collect()\n#> # A tibble: 18 × 2\n#>   CheckoutYear Checkouts\n#>          <int>     <int>\n#> 1         2005   3798685\n#> 2         2006   6599318\n#> 3         2007   7126627\n#> 4         2008   8438486\n#> 5         2009   9135167\n#> 6         2010   8608966\n#> # ℹ 12 more rows\n```\n:::\n\n\n\n\n\nGrazie ad arrow, questo codice funzionerà indipendentemente da quanto è grande il dataset sottostante.\nMa è attualmente piuttosto lento: sul computer di Hadley, ha impiegato ~10 secondi per essere eseguito.\nNon è terribile considerando quanti dati abbiamo, ma possiamo renderlo molto più veloce passando a un formato migliore.\n\n## Il formato parquet {#sec-parquet}\n\nPer rendere questi dati più facili da usare, passiamo al formato file parquet e dividiamoli in più file.\nLe sezioni seguenti ti introdurranno prima a parquet e al partizionamento, e poi applicheremo quello che abbiamo imparato ai dati della biblioteca di Seattle.\n\n### Vantaggi di parquet\n\nCome CSV, parquet è usato per dati rettangolari, ma invece di essere un formato testo che puoi leggere con qualsiasi editor di file, è un formato binario personalizzato progettato specificamente per le esigenze dei big data.\nQuesto significa che:\n\n-   I file parquet sono solitamente più piccoli del file CSV equivalente.\n    Parquet si basa su [codifiche efficienti](https://parquet.apache.org/docs/file-format/data-pages/encodings/) per mantenere bassa la dimensione del file, e supporta la compressione dei file.\n    Questo aiuta a rendere i file parquet veloci perché ci sono meno dati da spostare dal disco alla memoria.\n\n-   I file parquet hanno un sistema di tipi ricco.\n    Come abbiamo discusso in @sec-col-types, un file CSV non fornisce informazioni sui tipi di colonna.\n    Per esempio, un lettore CSV deve indovinare se `\"08-10-2022\"` dovrebbe essere analizzato come stringa o data.\n    Al contrario, i file parquet memorizzano i dati in un modo che registra il tipo insieme ai dati.\n\n-   I file parquet sono \"orientati alle colonne\".\n    Questo significa che sono organizzati colonna per colonna, molto come il data frame di R.\n    Questo porta tipicamente a prestazioni migliori per i compiti di analisi dati rispetto ai file CSV, che sono organizzati riga per riga.\n\n-   I file parquet sono \"suddivisi in chunk\", il che rende possibile lavorare su parti diverse del file allo stesso tempo, e, se sei fortunato, saltare completamente alcuni chunk.\n\nC'è uno svantaggio principale dei file parquet: non sono più \"leggibili dall'uomo\", cioè se guardi un file parquet usando `readr::read_file()`, vedrai solo un mucchio di caratteri senza senso.\n\n### Partizionamento\n\nMan mano che i dataset diventano sempre più grandi, memorizzare tutti i dati in un singolo file diventa sempre più doloroso ed è spesso utile dividere grandi dataset in molti file.\nQuando questa strutturazione è fatta intelligentemente, questa strategia può portare a miglioramenti significativi nelle prestazioni perché molte analisi richiederanno solo un sottoinsieme dei file.\n\nNon ci sono regole rigide e veloci su come partizionare il tuo dataset: i risultati dipenderanno dai tuoi dati, dai pattern di accesso e dai sistemi che leggono i dati.\nProbabilmente dovrai fare qualche sperimentazione prima di trovare il partizionamento ideale per la tua situazione.\nCome guida approssimativa, arrow suggerisce di evitare file più piccoli di 20MB e più grandi di 2GB ed evitare partizioni che producono più di 10.000 file.\nDovresti anche provare a partizionare per variabili su cui filtri; come vedrai a breve, questo permette ad arrow di saltare molto lavoro leggendo solo i file rilevanti.\n\n### Riscrivere i dati della biblioteca di Seattle\n\nApplichiamo queste idee ai dati della biblioteca di Seattle per vedere come funzionano in pratica.\nStiamo per partizionare per `CheckoutYear`, dato che è probabile che alcune analisi vogliano guardare solo i dati recenti e partizionare per anno produce 18 chunk di dimensione ragionevole.\n\nPer riscrivere i dati definiamo la partizione usando `dplyr::group_by()` e poi salviamo le partizioni in una directory con `arrow::write_dataset()`.\n`write_dataset()` ha due argomenti importanti: una directory dove creeremo i file e il formato che useremo.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npq_path <- \"data/seattle-library-checkouts\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = pq_path, format = \"parquet\")\n```\n:::\n\n\n\n\n\nQuesto richiede circa un minuto per essere eseguito; come vedremo a breve questo è un investimento iniziale che ripaga rendendo le operazioni future molto più veloci.\n\nDiamo un'occhiata a quello che abbiamo appena prodotto:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#> # A tibble: 18 × 2\n#>   files                            size_MB\n#>   <chr>                              <dbl>\n#> 1 CheckoutYear=2005/part-0.parquet    108.\n#> 2 CheckoutYear=2006/part-0.parquet    161.\n#> 3 CheckoutYear=2007/part-0.parquet    175.\n#> 4 CheckoutYear=2008/part-0.parquet    192.\n#> 5 CheckoutYear=2009/part-0.parquet    211.\n#> 6 CheckoutYear=2010/part-0.parquet    219.\n#> # ℹ 12 more rows\n```\n:::\n\n\n\n\n\nIl nostro singolo file CSV di 9GB è stato riscritto in 18 file parquet.\nI nomi dei file usano una convenzione \"auto-descrittiva\" usata dal progetto [Apache Hive](https://hive.apache.org).\nLe partizioni in stile Hive nominano le cartelle con una convenzione \"chiave=valore\", quindi come potresti indovinare, la directory `CheckoutYear=2005` contiene tutti i dati dove `CheckoutYear` è 2005.\nOgni file è tra 100 e 300 MB e la dimensione totale è ora intorno a 4 GB, poco più della metà della dimensione del file CSV originale.\nQuesto è come ci aspettiamo dato che parquet è un formato molto più efficiente.\n\n## Usare dplyr con arrow\n\nOra che abbiamo creato questi file parquet, dovremo leggerli di nuovo.\nUsiamo di nuovo `open_dataset()`, ma questa volta gli diamo una directory:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(pq_path)\n```\n:::\n\n\n\n\n\nOra possiamo scrivere la nostra pipeline dplyr.\nPer esempio, potremmo contare il numero totale di libri presi in prestito in ogni mese per gli ultimi cinque anni:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\n\n\n\n\nScrivere codice dplyr per i dati arrow è concettualmente simile a dbplyr, @sec-import-databases: scrivi codice dplyr, che viene automaticamente trasformato in una query che la libreria Apache Arrow C++ capisce, che viene poi eseguita quando chiami `collect()`.\nSe stampiamo l'oggetto `query` possiamo vedere un po' di informazioni su quello che ci aspettiamo che Arrow restituisca quando ha luogo l'esecuzione:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> CheckoutMonth: int64\n#> TotalCheckouts: int64\n#> \n#> * Grouped by CheckoutYear\n#> * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\n\n\n\nE possiamo ottenere i risultati chiamando `collect()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery |> collect()\n#> # A tibble: 58 × 3\n#> # Groups:   CheckoutYear [5]\n#>   CheckoutYear CheckoutMonth TotalCheckouts\n#>          <int>         <int>          <int>\n#> 1         2018             1         355101\n#> 2         2018             2         309813\n#> 3         2018             3         344487\n#> 4         2018             4         330988\n#> 5         2018             5         318049\n#> 6         2018             6         341825\n#> # ℹ 52 more rows\n```\n:::\n\n\n\n\n\nCome dbplyr, arrow capisce solo alcune espressioni R, quindi potresti non essere in grado di scrivere esattamente lo stesso codice che scriveresti di solito.\nTuttavia, la lista di operazioni e funzioni supportate è abbastanza estesa e continua a crescere; trova una lista completa delle funzioni attualmente supportate in `?acero`.\n\n### Prestazioni {#sec-parquet-fast}\n\nDiamo un'occhiata veloce all'impatto sulle prestazioni del passaggio da CSV a parquet.\nPrimo, cronometriamo quanto tempo ci vuole per calcolare il numero di libri presi in prestito in ogni mese del 2021, quando i dati sono memorizzati come un singolo CSV grande:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   8.415   0.932   8.110\n```\n:::\n\n\n\n\n\nOra usiamo la nostra nuova versione del dataset in cui i dati di prestito della biblioteca di Seattle sono stati partizionati in 18 file parquet più piccoli:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.183   0.034   0.054\n```\n:::\n\n\n\n\n\nL'accelerazione delle prestazioni di ~100x è attribuibile a due fattori: il partizionamento multi-file, e il formato dei singoli file:\n\n-   Il partizionamento migliora le prestazioni perché questa query usa `CheckoutYear == 2021` per filtrare i dati, e arrow è abbastanza intelligente da riconoscere che deve leggere solo 1 dei 18 file parquet.\n-   Il formato parquet migliora le prestazioni memorizzando i dati in un formato binario che può essere letto più direttamente in memoria. Il formato per colonne e i metadati ricchi significano che arrow deve leggere solo le quattro colonne effettivamente usate nella query (`CheckoutYear`, `MaterialType`, `CheckoutMonth`, e `Checkouts`).\n\nQuesta enorme differenza nelle prestazioni è il motivo per cui vale la pena convertire grandi CSV in parquet!\n\n### Usare duckdb con arrow\n\nC'è un ultimo vantaggio di parquet e arrow --- è molto facile trasformare un dataset arrow in un database DuckDB (@sec-import-databases) chiamando `arrow::to_duckdb()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <dbl>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\n\n\n\nLa cosa bella di `to_duckdb()` è che il trasferimento non comporta alcuna copia di memoria, e parla degli obiettivi dell'ecosistema arrow: abilitare transizioni senza soluzione di continuità da un ambiente di calcolo a un altro.\n\n### Esercizi\n\n1.  Scopri il libro più popolare ogni anno.\n2.  Quale autore ha più libri nel sistema bibliotecario di Seattle?\n3.  Come sono cambiati i prestiti di libri vs ebook negli ultimi 10 anni?\n\n## Riassunto\n\nIn questo capitolo, hai avuto un assaggio del pacchetto arrow, che fornisce un backend dplyr per lavorare con grandi dataset su disco.\nPuò lavorare con file CSV, ed è molto più veloce se converti i tuoi dati in parquet.\nParquet è un formato di dati binario progettato specificamente per l'analisi dei dati su computer moderni.\nMolti meno strumenti possono lavorare con file parquet rispetto a CSV, ma la sua struttura partizionata, compressa e colonnare lo rende molto più efficiente da analizzare.\n\nSuccessivamente imparerai la tua prima fonte di dati non rettangolare, che gestirai usando strumenti forniti dal pacchetto tidyr.\nCi concentreremo su dati che vengono da file JSON, ma i principi generali si applicano a dati ad albero indipendentemente dalla loro fonte.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}