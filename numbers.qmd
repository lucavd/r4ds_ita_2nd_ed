# Numeri {#sec-numbers}

```{r}
#| echo: false
source("_common.R")
```

## Introduzione

I vettori numerici sono la spina dorsale della data science, e li hai già usati molte volte in precedenza nel libro.
Ora è il momento di esaminare sistematicamente cosa puoi fare con essi in R, assicurandoti di essere ben posizionato per affrontare qualsiasi problema futuro che coinvolga vettori numerici.

Inizieremo dandoti un paio di strumenti per creare numeri se hai stringhe, e poi entreremo in un po' più di dettaglio su `count()`.
Poi ci immergeremo in varie trasformazioni numeriche che si abbinano bene con `mutate()`, incluse trasformazioni più generali che possono essere applicate ad altri tipi di vettori, ma sono spesso usate con vettori numerici.
Finiremo coprendo le funzioni di riassunto che si abbinano bene con `summarize()` e ti mostreremo come possono anche essere usate con `mutate()`.

### Prerequisiti

Questo capitolo usa principalmente funzioni di base R, che sono disponibili senza caricare alcun pacchetto.
Ma abbiamo ancora bisogno del tidyverse perché useremo queste funzioni di base R all'interno di funzioni tidyverse come `mutate()` e `filter()`.
Come nell'ultimo capitolo, useremo esempi reali da nycflights13, così come esempi giocattolo fatti con `c()` e `tribble()`.

```{r}
#| label: setup
#| message: false
library(tidyverse)
library(nycflights13)
```

## Creare numeri

Nella maggior parte dei casi, otterrai numeri già registrati in uno dei tipi numerici di R: intero o double.
In alcuni casi, tuttavia, li incontrerai come stringhe, possibilmente perché li hai creati ruotando dalle intestazioni delle colonne o perché qualcosa è andato storto nel tuo processo di importazione dati.

readr fornisce due funzioni utili per analizzare le stringhe in numeri: `parse_double()` e `parse_number()`.
Usa `parse_double()` quando hai numeri che sono stati scritti come stringhe:

```{r}
x <- c("1.2", "5.6", "1e3")
parse_double(x)
```

Usa `parse_number()` quando la stringa contiene testo non numerico che vuoi ignorare.
Questo è particolarmente utile per dati di valuta e percentuali:

```{r}
x <- c("$1,234", "USD 3,513", "59%")
parse_number(x)
```

## Conteggi {#sec-counts}

È sorprendente quanta data science puoi fare con solo conteggi e un po' di aritmetica di base, quindi dplyr si sforza di rendere il conteggio il più facile possibile con `count()`.
Questa funzione è ottima per esplorazione rapida e controlli durante l'analisi:

```{r}
flights |> count(dest)
```

(Nonostante il consiglio in @sec-workflow-style, di solito mettiamo `count()` su una singola riga perché è di solito usato alla console per un controllo rapido che un calcolo stia funzionando come previsto.)

Se vuoi vedere i valori più comuni, aggiungi `sort = TRUE`:

```{r}
flights |> count(dest, sort = TRUE)
```

E ricorda che se vuoi vedere tutti i valori, puoi usare `|> View()` o `|> print(n = Inf)`.

Puoi eseguire lo stesso calcolo "a mano" con `group_by()`, `summarize()` e `n()`.
Questo è utile perché ti permette di calcolare altri riassunti allo stesso tempo:

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE)
  )
```

`n()` è una funzione di riassunto speciale che non prende argomenti e invece accede alle informazioni sul gruppo "corrente".
Questo significa che funziona solo all'interno dei verbi dplyr:

```{r}
#| error: true
n()
```

Ci sono un paio di varianti di `n()` e `count()` che potresti trovare utili:

-   `n_distinct(x)` conta il numero di valori distinti (unici) di una o più variabili.
    Per esempio, potremmo capire quali destinazioni sono servite dal maggior numero di vettori:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(carriers = n_distinct(carrier)) |> 
      arrange(desc(carriers))
    ```

-   Un conteggio pesato è una somma.
    Per esempio potresti "contare" il numero di miglia che ogni aereo ha volato:

    ```{r}
    flights |> 
      group_by(tailnum) |> 
      summarize(miles = sum(distance))
    ```

    I conteggi pesati sono un problema comune quindi `count()` ha un argomento `wt` che fa la stessa cosa:

    ```{r}
    #| results: false
    flights |> count(tailnum, wt = distance)
    ```

-   Puoi contare i valori mancanti combinando `sum()` e `is.na()`.
    Nel dataset `flights` questo rappresenta voli che sono cancellati:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(n_cancelled = sum(is.na(dep_time))) 
    ```

### Esercizi

1.  Come puoi usare `count()` per contare il numero di righe con un valore mancante per una data variabile?
2.  Espandi le seguenti chiamate a `count()` per usare invece `group_by()`, `summarize()`, e `arrange()`:
    1.  `flights |> count(dest, sort = TRUE)`

    2.  `flights |> count(tailnum, wt = distance)`

## Trasformazioni numeriche

Le funzioni di trasformazione funzionano bene con `mutate()` perché il loro output ha la stessa lunghezza dell'input.
La grande maggioranza delle funzioni di trasformazione sono già integrate in base R.
È impraticabile elencarle tutte quindi questa sezione mostrerà quelle più utili.
Come esempio, mentre R fornisce tutte le funzioni trigonometriche che potresti sognare, non le elenchiamo qui perché sono raramente necessarie per la data science.

### Aritmetica e regole di riciclo {#sec-recycling}

Abbiamo introdotto le basi dell'aritmetica (`+`, `-`, `*`, `/`, `^`) in @sec-workflow-basics e le abbiamo usate molto da allora.
Queste funzioni non hanno bisogno di una spiegazione enorme perché fanno quello che hai imparato alle elementari.
Ma dobbiamo parlare brevemente delle **regole di riciclo** che determinano cosa succede quando i lati sinistro e destro hanno lunghezze diverse.
Questo è importante per operazioni come `flights |> mutate(air_time = air_time / 60)` perché ci sono 336.776 numeri a sinistra di `/` ma solo uno a destra.

R gestisce le lunghezze non corrispondenti **riciclando**, o ripetendo, il vettore corto.
Possiamo vedere questo in azione più facilmente se creiamo alcuni vettori fuori da un data frame:

```{r}
x <- c(1, 2, 10, 20)
x / 5
# è abbreviazione per
x / c(5, 5, 5, 5)
```

In generale, vuoi riciclare solo singoli numeri (cioè vettori di lunghezza 1), ma R riciclerà qualsiasi vettore di lunghezza più corta.
Di solito (ma non sempre) ti dà un avvertimento se il vettore più lungo non è un multiplo di quello più corto:

```{r}
x * c(1, 2)
x * c(1, 2, 3)
```

Queste regole di riciclo sono anche applicate ai confronti logici (`==`, `<`, `<=`, `>`, `>=`, `!=`) e possono portare a un risultato sorprendente se usi accidentalmente `==` invece di `%in%` e il data frame ha un numero sfortunato di righe.
Per esempio, prendi questo codice che tenta di trovare tutti i voli a gennaio e febbraio:

```{r}
flights |> 
  filter(month == c(1, 2))
```

Il codice viene eseguito senza errori, ma non restituisce quello che vuoi.
A causa delle regole di riciclo trova voli in righe con numeri dispari partiti a gennaio e voli in righe con numeri pari partiti a febbraio.
E sfortunatamente non c'è avvertimento perché `flights` ha un numero pari di righe.

Per proteggerti da questo tipo di fallimento silenzioso, la maggior parte delle funzioni tidyverse usa una forma più rigorosa di riciclo che ricicla solo valori singoli.
Sfortunatamente questo non aiuta qui, o in molti altri casi, perché il calcolo chiave è eseguito dalla funzione base R `==`, non `filter()`.

### Minimo e massimo

Le funzioni aritmetiche funzionano con coppie di variabili.
Due funzioni strettamente correlate sono `pmin()` e `pmax()`, che quando date due o più variabili restituiranno il valore più piccolo o più grande in ogni riga:

```{r}
df <- tribble(
  ~x, ~y,
  1,  3,
  5,  2,
  7, NA,
)

df |> 
  mutate(
    min = pmin(x, y, na.rm = TRUE),
    max = pmax(x, y, na.rm = TRUE)
  )
```

Nota che queste sono diverse dalle funzioni di riassunto `min()` e `max()` che prendono osservazioni multiple e restituiscono un singolo valore.
Puoi capire che hai usato la forma sbagliata quando tutti i minimi e tutti i massimi hanno lo stesso valore:

```{r}
df |> 
  mutate(
    min = min(x, y, na.rm = TRUE),
    max = max(x, y, na.rm = TRUE)
  )
```

### Aritmetica modulare

L'aritmetica modulare è il nome tecnico per il tipo di matematica che facevi prima di imparare le posizioni decimali, cioè divisione che produce un numero intero e un resto.
In R, `%/%` fa la divisione intera e `%%` calcola il resto:

```{r}
1:10 %/% 3
1:10 %% 3
```

L'aritmetica modulare è utile per il dataset `flights`, perché possiamo usarla per spacchettare la variabile `sched_dep_time` in `hour` e `minute`:

```{r}
flights |> 
  mutate(
    hour = sched_dep_time %/% 100,
    minute = sched_dep_time %% 100,
    .keep = "used"
  )
```

Possiamo combinare questo con il trucco `mean(is.na(x))` da @sec-logical-summaries per vedere come la proporzione di voli cancellati varia nel corso della giornata.
I risultati sono mostrati in @fig-prop-cancelled.

```{r}
#| label: fig-prop-cancelled
#| fig-cap: | 
#|   A line plot with scheduled departure hour on the x-axis, and proportion
#|   of cancelled flights on the y-axis. Cancellations seem to accumulate
#|   over the course of the day until 8pm, very late flights are much
#|   less likely to be cancelled.
#| fig-alt: |
#|   A line plot showing how proportion of cancelled flights changes over
#|   the course of the day. The proportion starts low at around 0.5% at
#|   5am, then steadily increases over the course of the day until peaking
#|   at 4% at 7pm. The proportion of cancelled flights then drops rapidly
#|   getting down to around 1% by midnight.
flights |> 
  group_by(hour = sched_dep_time %/% 100) |> 
  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |> 
  filter(hour > 1) |> 
  ggplot(aes(x = hour, y = prop_cancelled)) +
  geom_line(color = "grey50") + 
  geom_point(aes(size = n))
```

### Logaritmi

I logaritmi sono una trasformazione incredibilmente utile per gestire dati che variano attraverso più ordini di grandezza e convertire crescita esponenziale in crescita lineare.
In R, hai una scelta di tre logaritmi: `log()` (il logaritmo naturale, base e), `log2()` (base 2), e `log10()` (base 10).
Raccomandiamo di usare `log2()` o `log10()`.
`log2()` è facile da interpretare perché una differenza di 1 sulla scala logaritmica corrisponde a raddoppiare sulla scala originale e una differenza di -1 corrisponde a dimezzare; mentre `log10()` è facile da ri-trasformare perché (es.) 3 è 10\^3 = 1000.
L'inverso di `log()` è `exp()`; per calcolare l'inverso di `log2()` o `log10()` dovrai usare `2^` o `10^`.

### Arrotondamento {#sec-rounding}

Usa `round(x)` per arrotondare un numero al più vicino intero:

```{r}
round(123.456)
```

Puoi controllare la precisione dell'arrotondamento con il secondo argomento, `digits`.
`round(x, digits)` arrotonda al più vicino `10^-n` quindi `digits = 2` arrotonderà al più vicino 0.01.
Questa definizione è utile perché implica che `round(x, -3)` arrotonderà al più vicino migliaio, che infatti fa:

```{r}
round(123.456, 2)  # due cifre
round(123.456, 1)  # una cifra
round(123.456, -1) # arrotonda alla decina più vicina
round(123.456, -2) # arrotonda al centinaio più vicino
```

C'è una stranezza con `round()` che sembra sorprendente a prima vista:

```{r}
round(c(1.5, 2.5))
```

`round()` usa quello che è conosciuto come "arrotondamento alla metà pari" o arrotondamento del banchiere: se un numero è a metà strada tra due interi, sarà arrotondato all'intero **pari**.
Questa è una buona strategia perché mantiene l'arrotondamento imparziale: metà di tutti gli 0.5 sono arrotondati per eccesso, e metà sono arrotondati per difetto.

`round()` è abbinato con `floor()` che arrotonda sempre per difetto e `ceiling()` che arrotonda sempre per eccesso:

```{r}
x <- 123.456

floor(x)
ceiling(x)
```

Queste funzioni non hanno un argomento `digits`, quindi puoi invece ridurre la scala, arrotondare, e poi riportare alla scala originale:

```{r}
# Arrotonda per difetto alle due cifre più vicine
floor(x / 0.01) * 0.01
# Arrotonda per eccesso alle due cifre più vicine
ceiling(x / 0.01) * 0.01
```

Puoi usare la stessa tecnica se vuoi `round()` a un multiplo di qualche altro numero:

```{r}
# Arrotonda al multiplo più vicino di 4
round(x / 4) * 4

# Arrotonda al più vicino 0.25
round(x / 0.25) * 0.25
```

### Tagliare numeri in intervalli

Usa `cut()`[^numbers-1] per suddividere (cioè raggruppare) un vettore numerico in secchielli discreti:

[^numbers-1]: ggplot2 fornisce alcuni aiuti per casi comuni in `cut_interval()`, `cut_number()`, e `cut_width()`.
    ggplot2 è ammettibilmente un posto strano per queste funzioni, ma sono utili come parte del calcolo dell'istogramma e sono state scritte prima che esistessero altre parti del tidyverse.

```{r}
x <- c(1, 2, 5, 10, 15, 20)
cut(x, breaks = c(0, 5, 10, 15, 20))
```

Le interruzioni non devono essere equamente distribuite:

```{r}
cut(x, breaks = c(0, 5, 10, 100))
```

Puoi opzionalmente fornire le tue `labels`.
Nota che dovrebbero esserci una `labels` in meno rispetto alle `breaks`.

```{r}
cut(x, 
  breaks = c(0, 5, 10, 15, 20), 
  labels = c("sm", "md", "lg", "xl")
)
```

Qualsiasi valore fuori dal range delle interruzioni diventerà `NA`:

```{r}
y <- c(NA, -10, 5, 10, 30)
cut(y, breaks = c(0, 5, 10, 15, 20))
```

Vedi la documentazione per altri argomenti utili come `right` e `include.lowest`, che controllano se gli intervalli sono `[a, b)` o `(a, b]` e se l'intervallo più basso dovrebbe essere `[a, b]`.

### Aggregazioni cumulative e scorrevoli {#sec-cumulative-and-rolling-aggregates}

Base R fornisce `cumsum()`, `cumprod()`, `cummin()`, `cummax()` per somme, prodotti, minimi e massimi correnti, o cumulativi.
dplyr fornisce `cummean()` per medie cumulative.
Le somme cumulative tendono ad apparire di più nella pratica:

```{r}
x <- 1:10
cumsum(x)
```

Se hai bisogno di aggregazioni scorrevoli o slittanti più complesse, prova il pacchetto [slider](https://slider.r-lib.org/).

### Esercizi

1.  Spiega a parole cosa fa ogni riga del codice usato per generare @fig-prop-cancelled.

2.  Quali funzioni trigonometriche fornisce R?
    Indovina alcuni nomi e consulta la documentazione.
    Usano gradi o radianti?

3.  Attualmente `dep_time` e `sched_dep_time` sono comodi da guardare, ma difficili da calcolare perché non sono realmente numeri continui.
    Puoi vedere il problema di base eseguendo il codice qui sotto: c'è un gap tra ogni ora.

    ```{r}
    #| eval: false
    flights |> 
      filter(month == 1, day == 1) |> 
      ggplot(aes(x = sched_dep_time, y = dep_delay)) +
      geom_point()
    ```

    Convertili in una rappresentazione più veritiera del tempo (ore frazionarie o minuti dalla mezzanotte).

4.  Arrotonda `dep_time` e `arr_time` ai cinque minuti più vicini.

## Trasformazioni generali

Le seguenti sezioni descrivono alcune trasformazioni generali che sono spesso usate con vettori numerici, ma possono essere applicate a tutti gli altri tipi di colonna.

### Classifiche

dplyr fornisce un numero di funzioni di classificazione ispirate da SQL, ma dovresti sempre iniziare con `dplyr::min_rank()`.
Usa il metodo tipico per gestire i pareggi, es., 1°, 2°, 2°, 4°.

```{r}
x <- c(1, 2, 2, 3, 4, NA)
min_rank(x)
```

Nota che i valori più piccoli ottengono i ranghi più bassi; usa `desc(x)` per dare ai valori più grandi i ranghi più piccoli:

```{r}
min_rank(desc(x))
```

Se `min_rank()` non fa quello di cui hai bisogno, guarda le varianti `dplyr::row_number()`, `dplyr::dense_rank()`, `dplyr::percent_rank()`, e `dplyr::cume_dist()`.
Vedi la documentazione per i dettagli.

```{r}
df <- tibble(x = x)
df |> 
  mutate(
    row_number = row_number(x),
    dense_rank = dense_rank(x),
    percent_rank = percent_rank(x),
    cume_dist = cume_dist(x)
  )
```

You can achieve many of the same results by picking the appropriate `ties.method` argument to base R's `rank()`; you'll probably also want to set `na.last = "keep"` to keep `NA`s as `NA`.

`row_number()` can also be used without any arguments when inside a dplyr verb.
In this case, it'll give the number of the "current" row.
When combined with `%%` or `%/%` this can be a useful tool for dividing data into similarly sized groups:

```{r}
df <- tibble(id = 1:10)

df |> 
  mutate(
    row0 = row_number() - 1,
    three_groups = row0 %% 3,
    three_in_each_group = row0 %/% 3
  )
```

### Offsets

`dplyr::lead()` and `dplyr::lag()` allow you to refer to the values just before or just after the "current" value.
They return a vector of the same length as the input, padded with `NA`s at the start or end:

```{r}
x <- c(2, 5, 11, 11, 19, 35)
lag(x)
lead(x)
```

-   `x - lag(x)` gives you the difference between the current and previous value.

    ```{r}
    x - lag(x)
    ```

-   `x == lag(x)` tells you when the current value changes.

    ```{r}
    x == lag(x)
    ```

You can lead or lag by more than one position by using the second argument, `n`.

### Consecutive identifiers

Sometimes you want to start a new group every time some event occurs.
For example, when you're looking at website data, it's common to want to break up events into sessions, where you begin a new session after a gap of more than `x` minutes since the last activity.
For example, imagine you have the times when someone visited a website:

```{r}
events <- tibble(
  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)
)

```

And you've computed the time between each event, and figured out if there's a gap that's big enough to qualify:

```{r}
events <- events |> 
  mutate(
    diff = time - lag(time, default = first(time)),
    has_gap = diff >= 5
  )
events
```

But how do we go from that logical vector to something that we can `group_by()`?
`cumsum()`, from @sec-cumulative-and-rolling-aggregates, comes to the rescue as gap, i.e. `has_gap` is `TRUE`, will increment `group` by one (@sec-numeric-summaries-of-logicals):

```{r}
events |> mutate(
  group = cumsum(has_gap)
)
```

Another approach for creating grouping variables is `consecutive_id()`, which starts a new group every time one of its arguments changes.
For example, inspired by [this stackoverflow question](https://stackoverflow.com/questions/27482712), imagine you have a data frame with a bunch of repeated values:

```{r}
df <- tibble(
  x = c("a", "a", "a", "b", "c", "c", "d", "e", "a", "a", "b", "b"),
  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)
)
```

If you want to keep the first row from each repeated `x`, you could use `group_by()`, `consecutive_id()`, and `slice_head()`:

```{r}
df |> 
  group_by(id = consecutive_id(x)) |> 
  slice_head(n = 1)
```

### Exercises

1.  Find the 10 most delayed flights using a ranking function.
    How do you want to handle ties?
    Carefully read the documentation for `min_rank()`.

2.  Which plane (`tailnum`) has the worst on-time record?

3.  What time of day should you fly if you want to avoid delays as much as possible?

4.  What does `flights |> group_by(dest) |> filter(row_number() < 4)` do?
    What does `flights |> group_by(dest) |> filter(row_number(dep_delay) < 4)` do?

5.  For each destination, compute the total minutes of delay.
    For each flight, compute the proportion of the total delay for its destination.

6.  Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave.
    Using `lag()`, explore how the average flight delay for an hour is related to the average delay for the previous hour.

    ```{r}
    #| results: false
    flights |> 
      mutate(hour = dep_time %/% 100) |> 
      group_by(year, month, day, hour) |> 
      summarize(
        dep_delay = mean(dep_delay, na.rm = TRUE),
        n = n(),
        .groups = "drop"
      ) |> 
      filter(n > 5)
    ```

7.  Look at each destination.
    Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)?
    Compute the air time of a flight relative to the shortest flight to that destination.
    Which flights were most delayed in the air?

8.  Find all destinations that are flown by at least two carriers.
    Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.

## Riassunti numerici

Solo usando i conteggi, le medie e le somme che abbiamo già introdotto puoi andare lontano, ma R fornisce molte altre funzioni di riassunto utili.
Ecco una selezione che potresti trovare utile.

### Centro

Finora, abbiamo principalmente usato `mean()` per riassumere il centro di un vettore di valori.
Come abbiamo visto in @sec-sample-size, poiché la media è la somma divisa per il conteggio, è sensibile anche solo a pochi valori insolitamente alti o bassi.
Un'alternativa è usare `median()`, che trova un valore che giace nel "mezzo" del vettore, cioè il 50% dei valori è sopra di esso e il 50% è sotto.
A seconda della forma della distribuzione della variabile che ti interessa, media o mediana potrebbero essere una misura migliore del centro.
Per esempio, per distribuzioni simmetriche generalmente riportiamo la media mentre per distribuzioni asimmetriche di solito riportiamo la mediana.

@fig-mean-vs-median confronta la media vs. la mediana del ritardo di partenza (in minuti) per ogni destinazione.
Il ritardo mediano è sempre più piccolo del ritardo medio perché i voli a volte partono con ore di ritardo, ma non partono mai con ore di anticipo.

```{r}
#| label: fig-mean-vs-median
#| fig-cap: |
#|   A scatterplot showing the differences of summarizing daily departure
#|   delay with median instead of mean.
#| fig-alt: |
#|   All points fall below a 45° line, meaning that the median delay is
#|   always less than the mean delay. Most points are clustered in a 
#|   dense region of mean [0, 20] and median [-5, 5]. As the mean delay
#|   increases, the spread of the median also increases. There are two
#|   outlying points with mean ~60, median ~30, and mean ~85, median ~55.
flights |>
  group_by(year, month, day) |>
  summarize(
    mean = mean(dep_delay, na.rm = TRUE),
    median = median(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |> 
  ggplot(aes(x = mean, y = median)) + 
  geom_abline(slope = 1, intercept = 0, color = "white", linewidth = 2) +
  geom_point()
```

You might also wonder about the **mode**, or the most common value.
This is a summary that only works well for very simple cases (which is why you might have learned about it in high school), but it doesn't work well for many real datasets.
If the data is discrete, there may be multiple most common values, and if the data is continuous, there might be no most common value because every value is ever so slightly different.
For these reasons, the mode tends not to be used by statisticians and there's no mode function included in base R[^numbers-2].

[^numbers-2]: The `mode()` function does something quite different!

### Minimum, maximum, and quantiles {#sec-min-max-summary}

What if you're interested in locations other than the center?
`min()` and `max()` will give you the largest and smallest values.
Another powerful tool is `quantile()` which is a generalization of the median: `quantile(x, 0.25)` will find the value of `x` that is greater than 25% of the values, `quantile(x, 0.5)` is equivalent to the median, and `quantile(x, 0.95)` will find the value that's greater than 95% of the values.

For the `flights` data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.

```{r}
flights |>
  group_by(year, month, day) |>
  summarize(
    max = max(dep_delay, na.rm = TRUE),
    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),
    .groups = "drop"
  )
```

### Spread

Sometimes you're not so interested in where the bulk of the data lies, but in how it is spread out.
Two commonly used summaries are the standard deviation, `sd(x)`, and the inter-quartile range, `IQR()`.
We won't explain `sd()` here since you're probably already familiar with it, but `IQR()` might be new --- it's `quantile(x, 0.75) - quantile(x, 0.25)` and gives you the range that contains the middle 50% of the data.

We can use this to reveal a small oddity in the `flights` data.
You might expect the spread of the distance between origin and destination to be zero, since airports are always in the same place.
But the code below reveals a data oddity for airport [EGE](https://en.wikipedia.org/wiki/Eagle_County_Regional_Airport):

```{r}
flights |> 
  group_by(origin, dest) |> 
  summarize(
    distance_iqr = IQR(distance), 
    n = n(),
    .groups = "drop"
  ) |> 
  filter(distance_iqr > 0)
```

### Distributions

It's worth remembering that all of the summary statistics described above are a way of reducing the distribution down to a single number.
This means that they're fundamentally reductive, and if you pick the wrong summary, you can easily miss important differences between groups.
That's why it's always a good idea to visualize the distribution before committing to your summary statistics.

@fig-flights-dist shows the overall distribution of departure delays.
The distribution is so skewed that we have to zoom in to see the bulk of the data.
This suggests that the mean is unlikely to be a good summary and we might prefer the median instead.

```{r}
#| echo: false
#| label: fig-flights-dist
#| fig-cap: |
#|   (Left) The histogram of the full data is extremely skewed making it
#|   hard to get any details. (Right) Zooming into delays of less than two
#|   hours makes it possible to see what's happening with the bulk of the
#|   observations.
#| fig-alt: |
#|   Two histograms of `dep_delay`. On the left, it's very hard to see
#|   any pattern except that there's a very large spike around zero, the
#|   bars rapidly decay in height, and for most of the plot, you can't
#|   see any bars because they are too short to see. On the right,
#|   where we've discarded delays of greater than two hours, we can
#|   see that the spike occurs slightly below zero (i.e. most flights
#|   leave a couple of minutes early), but there's still a very steep
#|   decay after that.
#| fig-asp: 0.5
library(patchwork)

full <- flights |>
  ggplot(aes(x = dep_delay)) + 
  geom_histogram(binwidth = 15, na.rm = TRUE)

delayed120 <- flights |>
  filter(dep_delay < 120) |> 
  ggplot(aes(x = dep_delay)) + 
  geom_histogram(binwidth = 5)

full + delayed120
```

It's also a good idea to check that distributions for subgroups resemble the whole.
In the following plot 365 frequency polygons of `dep_delay`, one for each day, are overlaid.
The distributions seem to follow a common pattern, suggesting it's fine to use the same summary for each day.

```{r}
#| fig-alt: |
#|   The distribution of `dep_delay` is highly right skewed with a strong
#|   peak slightly less than 0. The 365 frequency polygons are mostly 
#|   overlapping forming a thick black band.
flights |>
  filter(dep_delay < 120) |> 
  ggplot(aes(x = dep_delay, group = interaction(day, month))) + 
  geom_freqpoly(binwidth = 5, alpha = 1/5)
```

Don't be afraid to explore your own custom summaries specifically tailored for the data that you're working with.
In this case, that might mean separately summarizing the flights that left early vs. the flights that left late, or given that the values are so heavily skewed, you might try a log-transformation.
Finally, don't forget what you learned in @sec-sample-size: whenever creating numerical summaries, it's a good idea to include the number of observations in each group.

### Positions

There's one final type of summary that's useful for numeric vectors, but also works with every other type of value: extracting a value at a specific position: `first(x)`, `last(x)`, and `nth(x, n)`.

For example, we can find the first, fifth and last departure for each day:

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    first_dep = first(dep_time, na_rm = TRUE), 
    fifth_dep = nth(dep_time, 5, na_rm = TRUE),
    last_dep = last(dep_time, na_rm = TRUE)
  )
```

(NB: Because dplyr functions use `_` to separate components of function and arguments names, these functions use `na_rm` instead of `na.rm`.)

If you're familiar with `[`, which we'll come back to in @sec-subset-many, you might wonder if you ever need these functions.
There are three reasons: the `default` argument allows you to provide a default if the specified position doesn't exist, the `order_by` argument allows you to locally override the order of the rows, and the `na_rm` argument allows you to drop missing values.

Extracting values at positions is complementary to filtering on ranks.
Filtering gives you all variables, with each observation in a separate row:

```{r}
flights |> 
  group_by(year, month, day) |> 
  mutate(r = min_rank(sched_dep_time)) |> 
  filter(r %in% c(1, max(r)))
```

### With `mutate()`

As the names suggest, the summary functions are typically paired with `summarize()`.
However, because of the recycling rules we discussed in @sec-recycling they can also be usefully paired with `mutate()`, particularly when you want do some sort of group standardization.
For example:

-   `x / sum(x)` calculates the proportion of a total.
-   `(x - mean(x)) / sd(x)` computes a Z-score (standardized to mean 0 and sd 1).
-   `(x - min(x)) / (max(x) - min(x))` standardizes to range \[0, 1\].
-   `x / first(x)` computes an index based on the first observation.

### Exercises

1.  Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights.
    When is `mean()` useful?
    When is `median()` useful?
    When might you want to use something else?
    Should you use arrival delay or departure delay?
    Why might you want to use data from `planes`?

2.  Which destinations show the greatest variation in air speed?

3.  Create a plot to further explore the adventures of EGE.
    Can you find any evidence that the airport moved locations?
    Can you find another variable that might explain the difference?

## Riassunto

Sei già familiare con molti strumenti per lavorare con i numeri, e dopo aver letto questo capitolo ora sai come usarli in R.
Hai anche imparato una manciata di trasformazioni generali utili che sono comunemente, ma non esclusivamente, applicate ai vettori numerici come classifiche e offset.
Infine, hai lavorato attraverso un numero di riassunti numerici, e discusso alcune delle sfide statistiche che dovresti considerare.

Nei prossimi due capitoli, ci immergeremo nel lavorare con le stringhe con il pacchetto stringr.
Le stringhe sono un argomento importante quindi ottengono due capitoli, uno sui fondamenti delle stringhe e uno sulle espressioni regolari.