# Web scraping {#sec-scraping}

```{r}
#| echo: false
source("_common.R")
```

## Introduzione

Questo capitolo ti introduce alle basi del web scraping con [rvest](https://rvest.tidyverse.org).
Il web scraping è uno strumento molto utile per estrarre dati dalle pagine web.
Alcuni siti web offrono un'API, un insieme di richieste HTTP strutturate che restituiscono dati come JSON, che puoi gestire utilizzando le tecniche del @sec-rectangling.
Dove possibile, dovresti usare l'API[^webscraping-1], perché di solito ti darà dati più affidabili.
Sfortunatamente, tuttavia, la programmazione con API web è al di fuori dell'ambito di questo libro.
Invece, stiamo insegnando lo scraping, una tecnica che funziona indipendentemente dal fatto che un sito fornisca o meno un'API.

[^webscraping-1]: E molte API popolari hanno già pacchetti CRAN che le racchiudono, quindi inizia con un po' di ricerca prima!

In questo capitolo, discuteremo prima l'etica e gli aspetti legali dello scraping prima di immergerci nelle basi dell'HTML.
Imparerai poi le basi dei selettori CSS per localizzare elementi specifici sulla pagina, e come usare le funzioni rvest per ottenere dati da testo e attributi dall'HTML e portarli in R.
Discuteremo poi alcune tecniche per capire quale selettore CSS ti serve per la pagina che stai facendo scraping, prima di concludere con un paio di casi di studio e una breve discussione sui siti web dinamici.

### Prerequisiti

In questo capitolo, ci concentreremo sugli strumenti forniti da rvest.
rvest è un membro del tidyverse, ma non è un membro principale quindi dovrai caricarlo esplicitamente.
Caricheremo anche il tidyverse completo dato che lo troveremo generalmente utile per lavorare con i dati che abbiamo raccolto.

```{r}
#| label: setup
#| message: false
library(tidyverse)
library(rvest)
```

## Etica e legalità dello scraping

Prima di iniziare a discutere il codice di cui avrai bisogno per eseguire il web scraping, dobbiamo parlare se sia legale ed etico farlo.
Nel complesso, la situazione è complicata per quanto riguarda entrambi questi aspetti.

Gli aspetti legali dipendono molto da dove vivi.
Tuttavia, come principio generale, se i dati sono pubblici, non personali e fattuali, probabilmente non avrai problemi[^webscraping-2].
Questi tre fattori sono importanti perché sono collegati ai termini e condizioni del sito, alle informazioni personali identificabili e al copyright, come discuteremo di seguito.

[^webscraping-2]: Ovviamente non siamo avvocati, e questo non è un consiglio legale.
    Ma questo è il miglior riassunto che possiamo dare dopo aver letto molto su questo argomento.

Se i dati non sono pubblici, non personali o fattuali o stai facendo scraping dei dati specificamente per guadagnarci, dovrai parlare con un avvocato.
In ogni caso, dovresti essere rispettoso delle risorse del server che ospita le pagine di cui stai facendo scraping.
Soprattutto, questo significa che se stai facendo scraping di molte pagine, dovresti assicurarti di aspettare un po' tra ogni richiesta.
Un modo semplice per farlo è usare il pacchetto [**polite**](https://dmi3kno.github.io/polite/) di Dmytro Perepolkin.
Farà automaticamente una pausa tra le richieste e metterà in cache i risultati così non chiederai mai la stessa pagina due volte.

### Termini di servizio

Se guardi attentamente, troverai che molti siti web includono un link "termini e condizioni" o "termini di servizio" da qualche parte nella pagina, e se leggi attentamente quella pagina spesso scoprirai che il sito proibisce specificamente il web scraping.
Queste pagine tendono ad essere una presa di territorio legale dove le aziende fanno affermazioni molto ampie.
È educato rispettare questi termini di servizio dove possibile, ma prendi qualsiasi affermazione con un pizzico di sale.

I tribunali statunitensi hanno generalmente trovato che semplicemente mettere i termini di servizio nel footer del sito web non è sufficiente perché tu sia vincolato da essi, ad es., [HiQ Labs v. LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn).
Generalmente, per essere vincolato ai termini di servizio, devi aver intrapreso un'azione esplicita come creare un account o spuntare una casella.
Questo è il motivo per cui il fatto che i dati siano **pubblici** è importante; se non hai bisogno di un account per accedervi, è improbabile che tu sia vincolato ai termini di servizio.
Nota, tuttavia, che la situazione è piuttosto diversa in Europa dove i tribunali hanno trovato che i termini di servizio sono applicabili anche se non accetti esplicitamente ad essi.

### Informazioni personali identificabili

Anche se i dati sono pubblici, dovresti essere estremamente attento riguardo al fare scraping di informazioni personali identificabili come nomi, indirizzi email, numeri di telefono, date di nascita, ecc.
L'Europa ha leggi particolarmente severe sulla raccolta o memorizzazione di tali dati ([GDPR](https://gdpr-info.eu/)), e indipendentemente da dove vivi probabilmente stai entrando in un pantano etico.
Ad esempio, nel 2016, un gruppo di ricercatori ha fatto scraping di informazioni di profilo pubbliche (ad es., nomi utente, età, genere, posizione, ecc.) su 70.000 persone sul sito di appuntamenti OkCupid e hanno rilasciato pubblicamente questi dati senza alcun tentativo di anonimizzazione.
Mentre i ricercatori sentivano che non c'era nulla di sbagliato in questo dato che i dati erano già pubblici, questo lavoro è stato ampiamente condannato a causa di preoccupazioni etiche riguardo all'identificabilità degli utenti le cui informazioni sono state rilasciate nel dataset.
Se il tuo lavoro coinvolge il fare scraping di informazioni personali identificabili, raccomandiamo fortemente di leggere sullo studio OkCupid[^webscraping-3] così come studi simili con etica di ricerca discutibile che coinvolgono l'acquisizione e il rilascio di informazioni personali identificabili.

[^webscraping-3]: Un esempio di articolo sullo studio OkCupid è stato pubblicato da Wired, <https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science>.

### Copyright

Infine, devi anche preoccuparti della legge sul copyright.
La legge sul copyright è complicata, ma vale la pena dare un'occhiata alla [legge statunitense](https://www.law.cornell.edu/uscode/text/17/102) che descrive esattamente cosa è protetto: "\[...\] opere originali di autorialità fissate in qualsiasi mezzo tangibile di espressione, \[...\]".
Poi continua a descrivere categorie specifiche a cui si applica come opere letterarie, opere musicali, film e altro.
Notevolmente assenti dalla protezione del copyright sono i dati.
Questo significa che finché limiti il tuo scraping ai fatti, la protezione del copyright non si applica.
(Ma nota che l'Europa ha un diritto "[sui generis](https://en.wikipedia.org/wiki/Database_right)" separato che protegge i database.)

Come breve esempio, negli Stati Uniti, liste di ingredienti e istruzioni non sono protette da copyright, quindi il copyright non può essere usato per proteggere una ricetta.
Ma se quella lista di ricette è accompagnata da sostanziale contenuto letterario nuovo, quello è protetto da copyright.
Questo è il motivo per cui quando cerchi una ricetta su internet c'è sempre così tanto contenuto prima.

Se hai bisogno di fare scraping di contenuto originale (come testo o immagini), potresti ancora essere protetto sotto la [dottrina del fair use](https://en.wikipedia.org/wiki/Fair_use).
Il fair use non è una regola rigida e veloce, ma pesa diversi fattori.
È più probabile che si applichi se stai raccogliendo i dati per ricerca o scopi non commerciali e se limiti quello che fai scraping solo a quello di cui hai bisogno.

## Basi di HTML

Per fare scraping delle pagine web, devi prima capire un po' di **HTML**, il linguaggio che descrive le pagine web.
HTML sta per **H**yper**T**ext **M**arkup **L**anguage e assomiglia a qualcosa del genere:

``` html
<html>
<head>
  <title>Page title</title>
</head>
<body>
  <h1 id='first'>A heading</h1>
  <p>Some text &amp; <b>some bold text.</b></p>
  <img src='myimg.png' width='100' height='100'>
</body>
```

L'HTML ha una struttura gerarchica formata da **elementi** che consistono di un tag di apertura (ad es., `<tag>`), **attributi** opzionali (`id='first'`), un tag di chiusura[^webscraping-4] (come `</tag>`), e **contenuti** (tutto quello che c'è tra il tag di apertura e di chiusura).

[^webscraping-4]: Diversi tag (inclusi `<p>` e `<li>`) non richiedono tag di chiusura, ma pensiamo sia meglio includerli perché rende più facile vedere la struttura dell'HTML.

Dato che `<` e `>` sono usati per i tag di apertura e chiusura, non puoi scriverli direttamente.
Invece devi usare gli **escape** HTML `&gt;` (maggiore di) e `&lt;` (minore di).
E dato che quegli escape usano `&`, se vuoi un ampersand letterale devi fare l'escape come `&amp;`.
C'è una vasta gamma di possibili escape HTML ma non devi preoccupartene troppo perché rvest li gestisce automaticamente per te.

Il web scraping è possibile perché la maggior parte delle pagine che contengono dati che vuoi fare scraping generalmente hanno una struttura consistente.

### Elementi

Ci sono oltre 100 elementi HTML.
Alcuni dei più importanti sono:

-   Ogni pagina HTML deve essere in un elemento `<html>`, e deve avere due figli: `<head>`, che contiene metadati del documento come il titolo della pagina, e `<body>`, che contiene il contenuto che vedi nel browser.

-   Tag di blocco come `<h1>` (heading 1), `<section>` (sezione), `<p>` (paragrafo), e `<ol>` (lista ordinata) formano la struttura complessiva della pagina.

-   Tag inline come `<b>` (grassetto), `<i>` (corsivo), e `<a>` (link) formattano il testo dentro i tag di blocco.

Se incontri un tag che non hai mai visto prima, puoi scoprire cosa fa con un po' di ricerca su Google.
Un altro buon posto per iniziare sono i [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML) che descrivono praticamente ogni aspetto della programmazione web.

La maggior parte degli elementi può avere contenuto tra i loro tag di apertura e chiusura.
Questo contenuto può essere testo o più elementi.
Ad esempio, il seguente HTML contiene un paragrafo di testo, con una parola in grassetto.

```         
<p>
  Hi! My <b>name</b> is Hadley.
</p>
```

I **figli** sono gli elementi che contiene, quindi l'elemento `<p>` sopra ha un figlio, l'elemento `<b>`.
L'elemento `<b>` non ha figli, ma ha contenuti (il testo "name").

### Attributi

I tag possono avere **attributi** nominati che assomigliano a `name1='value1' name2='value2'`.
Due degli attributi più importanti sono `id` e `class`, che sono usati in congiunzione con CSS (Cascading Style Sheets) per controllare l'aspetto visivo della pagina.
Questi sono spesso utili quando si fa scraping di dati da una pagina.
Gli attributi sono anche usati per registrare la destinazione dei link (l'attributo `href` degli elementi `<a>`) e la fonte delle immagini (l'attributo `src` dell'elemento `<img>`).

## Estrazione dati

Per iniziare a fare scraping, avrai bisogno dell'URL della pagina di cui vuoi fare scraping, che puoi solitamente copiare dal tuo browser web.
Dovrai poi leggere l'HTML per quella pagina in R con `read_html()`.
Questo restituisce un oggetto `xml_document`[^webscraping-5] che poi manipolerai usando le funzioni rvest:

[^webscraping-5]: Questa classe proviene dal pacchetto [xml2](https://xml2.r-lib.org).
    xml2 è un pacchetto di basso livello su cui rvest è costruito.

```{r}
html <- read_html("http://rvest.tidyverse.org/")
html
```

rvest include anche una funzione che ti permette di scrivere HTML inline.
La useremo molto in questo capitolo mentre insegniamo come le varie funzioni rvest funzionano con esempi semplici.

```{r}
html <- minimal_html("
  <p>This is a paragraph</p>
  <ul>
    <li>This is a bulleted list</li>
  </ul>
")
html
```

Ora che hai l'HTML in R, è tempo di estrarre i dati di interesse.
Imparerai prima sui selettori CSS che ti permettono di identificare gli elementi di interesse e le funzioni rvest che puoi usare per estrarre dati da loro.
Poi copriremo brevemente le tabelle HTML, che hanno alcuni strumenti speciali.

### Trovare elementi

CSS è l'abbreviazione di cascading style sheets, ed è uno strumento per definire lo stile visivo dei documenti HTML.
CSS include un linguaggio in miniatura per selezionare elementi su una pagina chiamato **selettori CSS**.
I selettori CSS definiscono pattern per localizzare elementi HTML, e sono utili per lo scraping perché forniscono un modo conciso di descrivere quali elementi vuoi estrarre.

Torneremo sui selettori CSS più dettagliatamente nel @sec-css-selectors, ma fortunatamente puoi andare molto lontano con solo tre:

-   `p` seleziona tutti gli elementi `<p>`.

-   `.title` seleziona tutti gli elementi con `class` "title".

-   `#title` seleziona l'elemento con l'attributo `id` che equivale "title".
    Gli attributi Id devono essere unici all'interno di un documento, quindi questo selezionerà sempre solo un singolo elemento.

Proviamo questi selettori con un esempio semplice:

```{r}
html <- minimal_html("
  <h1>This is a heading</h1>
  <p id='first'>This is a paragraph</p>
  <p class='important'>This is an important paragraph</p>
")
```

Usa `html_elements()` per trovare tutti gli elementi che corrispondono al selettore:

```{r}
html |> html_elements("p")
html |> html_elements(".important")
html |> html_elements("#first")
```

Un'altra funzione importante è `html_element()` che restituisce sempre lo stesso numero di output degli input.
Se la applichi a un documento intero ti darà la prima corrispondenza:

```{r}
html |> html_element("p")
```

C'è una differenza importante tra `html_element()` e `html_elements()` quando usi un selettore che non corrisponde a nessun elemento.
`html_elements()` restituisce un vettore di lunghezza 0, dove `html_element()` restituisce un valore mancante.
Questo sarà importante tra poco.

```{r}
html |> html_elements("b")
html |> html_element("b")
```

### Selezioni annidate

Nella maggior parte dei casi, userai `html_elements()` e `html_element()` insieme, tipicamente usando `html_elements()` per identificare elementi che diventeranno osservazioni poi usando `html_element()` per trovare elementi che diventeranno variabili.
Vediamo questo in azione usando un esempio semplice.
Qui abbiamo una lista non ordinata (`<ul>`) dove ogni elemento della lista (`<li>`) contiene alcune informazioni su quattro personaggi di StarWars:

```{r}
html <- minimal_html("
  <ul>
    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>
    <li><b>R4-P17</b> is a <i>droid</i></li>
    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>
    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>
  </ul>
  ")
```

Possiamo usare `html_elements()` per fare un vettore dove ogni elemento corrisponde a un personaggio diverso:

```{r}
characters <- html |> html_elements("li")
characters
```

Per estrarre il nome di ogni personaggio, usiamo `html_element()`, perché quando applicato all'output di `html_elements()` è garantito che restituisca una risposta per elemento:

```{r}
characters |> html_element("b")
```

La distinzione tra `html_element()` e `html_elements()` non è importante per il nome, ma è importante per il peso.
Vogliamo ottenere un peso per ogni personaggio, anche se non c'è un `<span>` del peso.
Questo è quello che fa `html_element()`:

```{r}
characters |> html_element(".weight")
```

`html_elements()` trova tutti gli `<span>` del peso che sono figli di `characters`.
Ce ne sono solo tre, quindi perdiamo la connessione tra nomi e pesi:

```{r}
characters |> html_elements(".weight")
```

Ora che hai selezionato gli elementi di interesse, dovrai estrarre i dati, sia dal contenuto testuale che da alcuni attributi.

### Testo e attributi

`html_text2()`[^webscraping-6] estrae il contenuto testuale semplice di un elemento HTML:

[^webscraping-6]: rvest fornisce anche `html_text()` ma dovresti quasi sempre usare `html_text2()` dato che fa un lavoro migliore nel convertire HTML annidato in testo.

```{r}
characters |> 
  html_element("b") |> 
  html_text2()

characters |> 
  html_element(".weight") |> 
  html_text2()
```

Nota che qualsiasi escape sarà gestito automaticamente; vedrai escape HTML solo nell'HTML sorgente, non nei dati restituiti da rvest.

`html_attr()` estrae dati dagli attributi:

```{r}
html <- minimal_html("
  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>
  <p><a href='https://en.wikipedia.org/wiki/Dog'>dogs</a></p>
")

html |> 
  html_elements("p") |> 
  html_element("a") |> 
  html_attr("href")
```

`html_attr()` restituisce sempre una stringa, quindi se stai estraendo numeri o date, dovrai fare un po' di post-elaborazione.

### Tabelle

Se sei fortunato, i tuoi dati saranno già memorizzati in una tabella HTML, e sarà una questione di leggerli da quella tabella.
È solitamente semplice riconoscere una tabella nel tuo browser: avrà una struttura rettangolare di righe e colonne, e puoi copiarla e incollarla in uno strumento come Excel.

Le tabelle HTML sono costruite da quattro elementi principali: `<table>`, `<tr>` (riga della tabella), `<th>` (intestazione della tabella), e `<td>` (dati della tabella).
Ecco una semplice tabella HTML con due colonne e tre righe:

```{r}
html <- minimal_html("
  <table class='mytable'>
    <tr><th>x</th>   <th>y</th></tr>
    <tr><td>1.5</td> <td>2.7</td></tr>
    <tr><td>4.9</td> <td>1.3</td></tr>
    <tr><td>7.2</td> <td>8.1</td></tr>
  </table>
  ")
```

rvest fornisce una funzione che sa come leggere questo tipo di dati: `html_table()`.
Restituisce una lista contenente un tibble per ogni tabella trovata sulla pagina.
Usa `html_element()` per identificare la tabella che vuoi estrarre:

```{r}
html |> 
  html_element(".mytable") |> 
  html_table()
```

Nota che `x` e `y` sono stati automaticamente convertiti in numeri.
Questa conversione automatica non funziona sempre, quindi in scenari più complessi potresti volerla disattivare con `convert = FALSE` e poi fare la tua conversione.

## Trovare i selettori giusti {#sec-css-selectors}

Capire il selettore di cui hai bisogno per i tuoi dati è tipicamente la parte più difficile del problema.
Spesso dovrai fare alcuni esperimenti per trovare un selettore che sia sia specifico (cioè non seleziona cose che non ti interessano) che sensibile (cioè seleziona tutto quello che ti interessa).
Molti tentativi ed errori sono una parte normale del processo!
Ci sono due strumenti principali disponibili per aiutarti con questo processo: SelectorGadget e gli strumenti di sviluppo del tuo browser.

[SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html) è un bookmarklet javascript che genera automaticamente selettori CSS basati sugli esempi positivi e negativi che fornisci.
Non funziona sempre, ma quando funziona, è magico!
Puoi imparare come installare e usare SelectorGadget leggendo <https://rvest.tidyverse.org/articles/selectorgadget.html> o guardando il video di Mine su <https://www.youtube.com/watch?v=PetWV5g1Xsc>.

Ogni browser moderno viene con qualche toolkit per sviluppatori, ma raccomandiamo Chrome, anche se non è il tuo browser abituale: i suoi strumenti di sviluppo web sono alcuni dei migliori e sono immediatamente disponibili.
Fai clic destro su un elemento sulla pagina e clicca `Inspect`.
Questo aprirà una vista espandibile della pagina HTML completa, centrata sull'elemento che hai appena cliccato.
Puoi usare questo per esplorare la pagina e avere un'idea di quali selettori potrebbero funzionare.
Presta particolare attenzione agli attributi class e id, dato che questi sono spesso usati per formare la struttura visiva della pagina, e quindi costituiscono buoni strumenti per estrarre i dati che stai cercando.

All'interno della vista Elements, puoi anche fare clic destro su un elemento e scegliere `Copy as Selector` per generare un selettore che identificherà univocamente l'elemento di interesse.

Se SelectorGadget o Chrome DevTools hanno generato un selettore CSS che non capisci, prova [Selectors Explained](https://kittygiraudel.github.io/selectors-explained/){.uri} che traduce i selettori CSS in inglese semplice.
Se ti ritrovi a farlo spesso, potresti voler imparare di più sui selettori CSS in generale.
Raccomandiamo di iniziare con il tutorial divertente [CSS dinner](https://flukeout.github.io/) e poi fare riferimento ai [MDN web docs](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors).

## Mettere tutto insieme

Mettiamo tutto insieme per fare scraping di alcuni siti web.
C'è qualche rischio che questi esempi possano non funzionare più quando li esegui --- questa è la sfida fondamentale del web scraping; se la struttura del sito cambia, allora dovrai cambiare il tuo codice di scraping.

### StarWars

rvest include un esempio molto semplice in `vignette("starwars")`.
Questa è una pagina semplice con HTML minimale quindi è un buon posto per iniziare.
Ti incoraggio a navigare a quella pagina ora e usare "Inspect Element" per ispezionare una delle intestazioni che è il titolo di un film di Star Wars.
Usa la tastiera o il mouse per esplorare la gerarchia dell'HTML e vedi se riesci a capire la struttura condivisa usata da ogni film.

Dovresti essere in grado di vedere che ogni film ha una struttura condivisa che assomiglia a questa:

``` html
<section>
  <h2 data-id="1">The Phantom Menace</h2>
  <p>Released: 1999-05-19</p>
  <p>Director: <span class="director">George Lucas</span></p>
  
  <div class="crawl">
    <p>...</p>
    <p>...</p>
    <p>...</p>
  </div>
</section>
```

Il nostro obiettivo è trasformare questi dati in un data frame di 7 righe con variabili `title`, `year`, `director`, e `intro`.
Inizieremo leggendo l'HTML ed estraendo tutti gli elementi `<section>`:

```{r}
url <- "https://rvest.tidyverse.org/articles/starwars.html"
html <- read_html(url)

section <- html |> html_elements("section")
section
```

Questo recupera sette elementi corrispondenti ai sette film trovati su quella pagina, suggerendo che usare `section` come selettore è buono.
Estrarre i singoli elementi è semplice dato che i dati si trovano sempre nel testo.
È solo una questione di trovare il selettore giusto:

```{r}
section |> html_element("h2") |> html_text2()

section |> html_element(".director") |> html_text2()
```

Una volta che abbiamo fatto questo per ogni componente, possiamo racchiudere tutti i risultati in un tibble:

```{r}
tibble(
  title = section |> 
    html_element("h2") |> 
    html_text2(),
  released = section |> 
    html_element("p") |> 
    html_text2() |> 
    str_remove("Released: ") |> 
    parse_date(),
  director = section |> 
    html_element(".director") |> 
    html_text2(),
  intro = section |> 
    html_element(".crawl") |> 
    html_text2()
)
```

Abbiamo fatto un po' di elaborazione in più di `released` per ottenere una variabile che sarà facile da usare più tardi nella nostra analisi.

### Film migliori di IMDB

Per il nostro prossimo compito affronteremo qualcosa di un po' più difficile, estrarre i 250 film migliori dall'internet movie database (IMDb).
Al momento in cui abbiamo scritto questo capitolo, la pagina assomigliava a @fig-scraping-imdb.

```{r}
#| label: fig-scraping-imdb
#| echo: false
#| fig-cap: | 
#|   Screenshot della pagina web dei film migliori di IMDb presa il 2022-12-05.
#| fig-alt: |
#|   Lo screenshot mostra una tabella con colonne "Rank and Title",
#|   "IMDb Rating", e "Your Rating". 9 film sui migliori 250
#|   sono mostrati. I migliori 5 sono the Shawshank Redemption, The Godfather,
#|   The Dark Knight, The Godfather: Part II, e 12 Angry Men.
knitr::include_graphics("screenshots/scraping-imdb.png", dpi = 300)
```

Questi dati hanno una struttura tabellare chiara quindi vale la pena iniziare con `html_table()`:

```{r}
url <- "https://web.archive.org/web/20220201012049/https://www.imdb.com/chart/top/"
html <- read_html(url)

table <- html |> 
  html_element("table") |> 
  html_table()
table
```

Questo include alcune colonne vuote, ma nel complesso fa un buon lavoro nel catturare le informazioni dalla tabella.
Tuttavia, dobbiamo fare un po' più di elaborazione per renderlo più facile da usare.
Prima, rinomineremo le colonne per essere più facili da lavorare, e rimuoveremo gli spazi extra in rank e title.
Faremo questo con `select()` (invece di `rename()`) per fare la ridenominazione e selezione di solo queste due colonne in un passo.
Poi rimuoveremo le nuove linee e gli spazi extra, e poi applicheremo `separate_wider_regex()` (dal @sec-extract-variables) per estrarre il titolo, anno, e rank nelle loro variabili.

```{r}
ratings <- table |>
  select(
    rank_title_year = `Rank & Title`,
    rating = `IMDb Rating`
  ) |> 
  mutate(
    rank_title_year = str_replace_all(rank_title_year, "\n +", " ")
  ) |> 
  separate_wider_regex(
    rank_title_year,
    patterns = c(
      rank = "\\d+", "\\. ",
      title = ".+", " +\\(",
      year = "\\d+", "\\)"
    )
  )
ratings
```

Anche in questo caso dove la maggior parte dei dati proviene dalle celle della tabella, vale ancora la pena guardare l'HTML grezzo.
Se lo fai, scoprirai che possiamo aggiungere un po' di dati extra usando uno degli attributi.
Questo è uno dei motivi per cui vale la pena passare un po' di tempo esplorando la fonte della pagina; potresti trovare dati extra, o potresti trovare una via di parsing che è leggermente più facile.

```{r}
html |> 
  html_elements("td strong") |> 
  head() |> 
  html_attr("title")
```

Possiamo combinare questo con i dati tabellari e di nuovo applicare `separate_wider_regex()` per estrarre il pezzo di dati che ci interessa:

```{r}
ratings |>
  mutate(
    rating_n = html |> html_elements("td strong") |> html_attr("title")
  ) |> 
  separate_wider_regex(
    rating_n,
    patterns = c(
      "[0-9.]+ based on ",
      number = "[0-9,]+",
      " user ratings"
    )
  ) |> 
  mutate(
    number = parse_number(number)
  )
```

## Siti dinamici

Finora ci siamo concentrati su siti web dove `html_elements()` restituisce quello che vedi nel browser e abbiamo discusso come analizzare quello che restituisce e come organizzare quell'informazione in data frame ordinati.
Di tanto in tanto, tuttavia, incontrerai un sito dove `html_elements()` e amici non restituiscono nulla di simile a quello che vedi nel browser.
In molti casi, questo è perché stai cercando di fare scraping di un sito web che genera dinamicamente il contenuto della pagina con javascript.
Questo attualmente non funziona con rvest, perché rvest scarica l'HTML grezzo e non esegue javascript.

È ancora possibile fare scraping di questi tipi di siti, ma rvest ha bisogno di usare un processo più costoso: simulare completamente il browser web includendo l'esecuzione di tutto il javascript.
Questa funzionalità non è disponibile al momento della scrittura, ma è qualcosa su cui stiamo lavorando attivamente e potrebbe essere disponibile quando leggi questo.
Usa il [pacchetto chromote](https://rstudio.github.io/chromote/index.html) che esegue effettivamente il browser Chrome in background, e ti dà strumenti aggiuntivi per interagire con il sito, come un umano che digita testo e clicca pulsanti.
Controlla il [sito web di rvest](http://rvest.tidyverse.org/) per più dettagli.

## Riassunto

In questo capitolo, hai imparato sul perché, il perché no, e il come fare scraping di dati dalle pagine web.
Prima, hai imparato le basi dell'HTML e usare i selettori CSS per riferirsi a elementi specifici, poi hai imparato sull'usare il pacchetto rvest per ottenere dati dall'HTML in R.
Abbiamo poi dimostrato il web scraping con due casi di studio: uno scenario più semplice sul fare scraping di dati sui film di StarWars dal sito web del pacchetto rvest e uno scenario più complesso sul fare scraping dei 250 film migliori da IMDB.

I dettagli tecnici del fare scraping di dati dal web possono essere complessi, particolarmente quando si tratta di siti, tuttavia le considerazioni legali ed etiche possono essere ancora più complesse.
È importante per te educarti su entrambi questi prima di iniziare a fare scraping di dati.

Questo ci porta alla fine della parte di importazione del libro dove hai imparato tecniche per ottenere dati da dove vivono (fogli di calcolo, database, file JSON, e siti web) in una forma ordinata in R.
Ora è tempo di rivolgere i nostri sguardi a un nuovo argomento: ottenere il massimo da R come linguaggio di programmazione.