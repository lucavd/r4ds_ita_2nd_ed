---
freeze: true
---

# Arrow {#sec-arrow}

```{r}
#| echo: false

source("_common.R")
```

## Introduzione

I file CSV sono progettati per essere facilmente letti dagli esseri umani.
Sono un buon formato di interscambio perché sono molto semplici e possono essere letti da qualsiasi strumento esistente.
Ma i file CSV non sono molto efficienti: devi fare parecchio lavoro per leggere i dati in R.
In questo capitolo, imparerai un'alternativa potente: il [formato parquet](https://parquet.apache.org/), un formato basato su standard aperti ampiamente utilizzato dai sistemi di big data.

Abbineremo i file parquet con [Apache Arrow](https://arrow.apache.org), una toolbox multi-linguaggio progettata per l'analisi efficiente e il trasporto di grandi dataset.
Useremo Apache Arrow tramite il [pacchetto arrow](https://arrow.apache.org/docs/r/), che fornisce un backend dplyr permettendoti di analizzare dataset più grandi della memoria usando la sintassi familiare di dplyr.
Come beneficio aggiuntivo, arrow è estremamente veloce: vedrai alcuni esempi più avanti nel capitolo.

Sia arrow che dbplyr forniscono backend dplyr, quindi potresti chiederti quando usare ciascuno.
In molti casi, la scelta è fatta per te, poiché i dati sono già in un database o in file parquet, e vorrai lavorarci così come sono.
Ma se stai iniziando con i tuoi dati (magari file CSV), puoi caricarli in un database o convertirli in parquet.
In generale, è difficile sapere cosa funzionerà meglio, quindi nelle fasi iniziali della tua analisi ti incoraggiamo a provare entrambi e scegliere quello che funziona meglio per te.

(Un grande ringraziamento a Danielle Navarro che ha contribuito alla versione iniziale di questo capitolo.)

### Prerequisiti

In questo capitolo, continueremo a usare il tidyverse, in particolare dplyr, ma lo abbineremo al pacchetto arrow che è progettato specificamente per lavorare con grandi dati.

```{r setup}
#| message: false
#| warning: false
library(tidyverse)
library(arrow)
```

Più avanti nel capitolo, vedremo anche alcune connessioni tra arrow e duckdb, quindi avremo bisogno anche di dbplyr e duckdb.

```{r}
library(dbplyr, warn.conflicts = FALSE)
library(duckdb)
```

## Ottenere i dati

Iniziamo ottenendo un dataset degno di questi strumenti: un dataset di prestiti di oggetti dalle biblioteche pubbliche di Seattle, disponibile online su [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).
Questo dataset contiene 41.389.465 righe che ti dicono quante volte ogni libro è stato preso in prestito ogni mese da aprile 2005 a ottobre 2022.

Il seguente codice ti darà una copia cached dei dati.
I dati sono un file CSV di 9GB, quindi ci vorrà del tempo per scaricarlo.
Raccomando vivamente di usare `curl::multi_download()` per ottenere file molto grandi poiché è costruito esattamente per questo scopo: ti dà una barra di progresso e può riprendere il download se viene interrotto.

```{r}
#| eval: !expr "!file.exists('data/seattle-library-checkouts.csv')"
dir.create("data", showWarnings = FALSE)

curl::multi_download(
  "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  "data/seattle-library-checkouts.csv",
  resume = TRUE
)
```

## Aprire un dataset

Iniziamo dando un'occhiata ai dati.
A 9 GB, questo file è abbastanza grande che probabilmente non vogliamo caricare tutto in memoria.
Una buona regola empirica è che di solito vuoi almeno il doppio della memoria rispetto alla dimensione dei dati, e molti laptop arrivano al massimo a 16 GB.
Questo significa che vogliamo evitare `read_csv()` e invece usare `arrow::open_dataset()`:

```{r open-dataset}
seattle_csv <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

Cosa succede quando questo codice viene eseguito?
`open_dataset()` scannerà alcune migliaia di righe per capire la struttura del dataset.
La colonna `ISBN` contiene valori vuoti per le prime 80.000 righe, quindi dobbiamo specificare il tipo di colonna per aiutare arrow a capire la struttura dei dati.
Una volta che i dati sono stati scansionati da `open_dataset()`, registra quello che ha trovato e si ferma; leggerà solo ulteriori righe quando le richiedi specificamente.
Questi metadati sono quello che vediamo se stampiamo `seattle_csv`:

```{r}
seattle_csv
```

La prima riga nell'output ti dice che `seattle_csv` è memorizzato localmente su disco come un singolo file CSV; sarà caricato in memoria solo quando necessario.
Il resto dell'output ti dice il tipo di colonna che arrow ha imputato per ogni colonna.

Possiamo vedere cosa c'è effettivamente dentro con `glimpse()`.
Questo rivela che ci sono ~41 milioni di righe e 12 colonne, e ci mostra alcuni valori.

```{r glimpse-data}
#| cache: true
seattle_csv |> glimpse()
```

Possiamo iniziare a usare questo dataset con i verbi dplyr, usando `collect()` per forzare arrow a eseguire il calcolo e restituire alcuni dati.
Per esempio, questo codice ci dice il numero totale di prestiti per anno:

```{r}
#| cache: true
seattle_csv |> 
  group_by(CheckoutYear) |> 
  summarise(Checkouts = sum(Checkouts)) |> 
  arrange(CheckoutYear) |> 
  collect()
```

Grazie ad arrow, questo codice funzionerà indipendentemente da quanto è grande il dataset sottostante.
Ma è attualmente piuttosto lento: sul computer di Hadley, ha impiegato ~10 secondi per essere eseguito.
Non è terribile considerando quanti dati abbiamo, ma possiamo renderlo molto più veloce passando a un formato migliore.

## Il formato parquet {#sec-parquet}

Per rendere questi dati più facili da usare, passiamo al formato file parquet e dividiamoli in più file.
Le sezioni seguenti ti introdurranno prima a parquet e al partizionamento, e poi applicheremo quello che abbiamo imparato ai dati della biblioteca di Seattle.

### Vantaggi di parquet

Come CSV, parquet è usato per dati rettangolari, ma invece di essere un formato testo che puoi leggere con qualsiasi editor di file, è un formato binario personalizzato progettato specificamente per le esigenze dei big data.
Questo significa che:

-   I file parquet sono solitamente più piccoli del file CSV equivalente.
    Parquet si basa su [codifiche efficienti](https://parquet.apache.org/docs/file-format/data-pages/encodings/) per mantenere bassa la dimensione del file, e supporta la compressione dei file.
    Questo aiuta a rendere i file parquet veloci perché ci sono meno dati da spostare dal disco alla memoria.

-   I file parquet hanno un sistema di tipi ricco.
    Come abbiamo discusso in @sec-col-types, un file CSV non fornisce informazioni sui tipi di colonna.
    Per esempio, un lettore CSV deve indovinare se `"08-10-2022"` dovrebbe essere analizzato come stringa o data.
    Al contrario, i file parquet memorizzano i dati in un modo che registra il tipo insieme ai dati.

-   I file parquet sono "orientati alle colonne".
    Questo significa che sono organizzati colonna per colonna, molto come il data frame di R.
    Questo porta tipicamente a prestazioni migliori per i compiti di analisi dati rispetto ai file CSV, che sono organizzati riga per riga.

-   I file parquet sono "suddivisi in chunk", il che rende possibile lavorare su parti diverse del file allo stesso tempo, e, se sei fortunato, saltare completamente alcuni chunk.

C'è uno svantaggio principale dei file parquet: non sono più "leggibili dall'uomo", cioè se guardi un file parquet usando `readr::read_file()`, vedrai solo un mucchio di caratteri senza senso.

### Partizionamento

Man mano che i dataset diventano sempre più grandi, memorizzare tutti i dati in un singolo file diventa sempre più doloroso ed è spesso utile dividere grandi dataset in molti file.
Quando questa strutturazione è fatta intelligentemente, questa strategia può portare a miglioramenti significativi nelle prestazioni perché molte analisi richiederanno solo un sottoinsieme dei file.

Non ci sono regole rigide e veloci su come partizionare il tuo dataset: i risultati dipenderanno dai tuoi dati, dai pattern di accesso e dai sistemi che leggono i dati.
Probabilmente dovrai fare qualche sperimentazione prima di trovare il partizionamento ideale per la tua situazione.
Come guida approssimativa, arrow suggerisce di evitare file più piccoli di 20MB e più grandi di 2GB ed evitare partizioni che producono più di 10.000 file.
Dovresti anche provare a partizionare per variabili su cui filtri; come vedrai a breve, questo permette ad arrow di saltare molto lavoro leggendo solo i file rilevanti.

### Riscrivere i dati della biblioteca di Seattle

Applichiamo queste idee ai dati della biblioteca di Seattle per vedere come funzionano in pratica.
Stiamo per partizionare per `CheckoutYear`, dato che è probabile che alcune analisi vogliano guardare solo i dati recenti e partizionare per anno produce 18 chunk di dimensione ragionevole.

Per riscrivere i dati definiamo la partizione usando `dplyr::group_by()` e poi salviamo le partizioni in una directory con `arrow::write_dataset()`.
`write_dataset()` ha due argomenti importanti: una directory dove creeremo i file e il formato che useremo.

```{r}
pq_path <- "data/seattle-library-checkouts"
```

```{r write-dataset}
#| eval: !expr "!file.exists(pq_path)"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = pq_path, format = "parquet")
```

Questo richiede circa un minuto per essere eseguito; come vedremo a breve questo è un investimento iniziale che ripaga rendendo le operazioni future molto più veloci.

Diamo un'occhiata a quello che abbiamo appena prodotto:

```{r show-parquet-files}
tibble(
  files = list.files(pq_path, recursive = TRUE),
  size_MB = file.size(file.path(pq_path, files)) / 1024^2
)
```

Il nostro singolo file CSV di 9GB è stato riscritto in 18 file parquet.
I nomi dei file usano una convenzione "auto-descrittiva" usata dal progetto [Apache Hive](https://hive.apache.org).
Le partizioni in stile Hive nominano le cartelle con una convenzione "chiave=valore", quindi come potresti indovinare, la directory `CheckoutYear=2005` contiene tutti i dati dove `CheckoutYear` è 2005.
Ogni file è tra 100 e 300 MB e la dimensione totale è ora intorno a 4 GB, poco più della metà della dimensione del file CSV originale.
Questo è come ci aspettiamo dato che parquet è un formato molto più efficiente.

## Usare dplyr con arrow

Ora che abbiamo creato questi file parquet, dovremo leggerli di nuovo.
Usiamo di nuovo `open_dataset()`, ma questa volta gli diamo una directory:

```{r}
seattle_pq <- open_dataset(pq_path)
```

Ora possiamo scrivere la nostra pipeline dplyr.
Per esempio, potremmo contare il numero totale di libri presi in prestito in ogni mese per gli ultimi cinque anni:

```{r books-by-year-query}
query <- seattle_pq |> 
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear, CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(CheckoutYear, CheckoutMonth)
```

Scrivere codice dplyr per i dati arrow è concettualmente simile a dbplyr, @sec-import-databases: scrivi codice dplyr, che viene automaticamente trasformato in una query che la libreria Apache Arrow C++ capisce, che viene poi eseguita quando chiami `collect()`.
Se stampiamo l'oggetto `query` possiamo vedere un po' di informazioni su quello che ci aspettiamo che Arrow restituisca quando ha luogo l'esecuzione:

```{r}
query
```

E possiamo ottenere i risultati chiamando `collect()`:

```{r books-by-year}
query |> collect()
```

Come dbplyr, arrow capisce solo alcune espressioni R, quindi potresti non essere in grado di scrivere esattamente lo stesso codice che scriveresti di solito.
Tuttavia, la lista di operazioni e funzioni supportate è abbastanza estesa e continua a crescere; trova una lista completa delle funzioni attualmente supportate in `?acero`.

### Prestazioni {#sec-parquet-fast}

Diamo un'occhiata veloce all'impatto sulle prestazioni del passaggio da CSV a parquet.
Primo, cronometriamo quanto tempo ci vuole per calcolare il numero di libri presi in prestito in ogni mese del 2021, quando i dati sono memorizzati come un singolo CSV grande:

```{r dataset-performance-csv}
#| cache: true

seattle_csv |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

Ora usiamo la nostra nuova versione del dataset in cui i dati di prestito della biblioteca di Seattle sono stati partizionati in 18 file parquet più piccoli:

```{r dataset-performance-multiple-parquet}
#| cache: true

seattle_pq |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

L'accelerazione delle prestazioni di ~100x è attribuibile a due fattori: il partizionamento multi-file, e il formato dei singoli file:

-   Il partizionamento migliora le prestazioni perché questa query usa `CheckoutYear == 2021` per filtrare i dati, e arrow è abbastanza intelligente da riconoscere che deve leggere solo 1 dei 18 file parquet.
-   Il formato parquet migliora le prestazioni memorizzando i dati in un formato binario che può essere letto più direttamente in memoria. Il formato per colonne e i metadati ricchi significano che arrow deve leggere solo le quattro colonne effettivamente usate nella query (`CheckoutYear`, `MaterialType`, `CheckoutMonth`, e `Checkouts`).

Questa enorme differenza nelle prestazioni è il motivo per cui vale la pena convertire grandi CSV in parquet!

### Usare duckdb con arrow

C'è un ultimo vantaggio di parquet e arrow --- è molto facile trasformare un dataset arrow in un database DuckDB (@sec-import-databases) chiamando `arrow::to_duckdb()`:

```{r use-duckdb}
seattle_pq |> 
  to_duckdb() |>
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutYear)) |>
  collect()
```

La cosa bella di `to_duckdb()` è che il trasferimento non comporta alcuna copia di memoria, e parla degli obiettivi dell'ecosistema arrow: abilitare transizioni senza soluzione di continuità da un ambiente di calcolo a un altro.

### Esercizi

1.  Scopri il libro più popolare ogni anno.
2.  Quale autore ha più libri nel sistema bibliotecario di Seattle?
3.  Come sono cambiati i prestiti di libri vs ebook negli ultimi 10 anni?

## Riassunto

In questo capitolo, hai avuto un assaggio del pacchetto arrow, che fornisce un backend dplyr per lavorare con grandi dataset su disco.
Può lavorare con file CSV, ed è molto più veloce se converti i tuoi dati in parquet.
Parquet è un formato di dati binario progettato specificamente per l'analisi dei dati su computer moderni.
Molti meno strumenti possono lavorare con file parquet rispetto a CSV, ma la sua struttura partizionata, compressa e colonnare lo rende molto più efficiente da analizzare.

Successivamente imparerai la tua prima fonte di dati non rettangolare, che gestirai usando strumenti forniti dal pacchetto tidyr.
Ci concentreremo su dati che vengono da file JSON, ma i principi generali si applicano a dati ad albero indipendentemente dalla loro fonte.
